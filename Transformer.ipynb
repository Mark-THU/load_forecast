{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mark-THU/load_forecast/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-jyRZqxUF2-"
      },
      "source": [
        "# prediction by Transformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sqlalchemy import create_engine\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXGB-NgvYQdT"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14JEOyI5UF3I"
      },
      "source": [
        "# load data\n",
        "# url = 'https://raw.githubusercontent.com/Mark-THU/load_forecast/main/integrate_0101510000.csv'\n",
        "url = 'https://raw.githubusercontent.com/Mark-THU/load_forecast/main/dataset.csv'\n",
        "data = pd.read_csv(url, sep='\\t', index_col='time')\n",
        "data = data[['tem', 'tembody', 'month_of_year', 'is_holiday', 'day_of_week', 'load']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMuY2BhfUF3K"
      },
      "source": [
        "# 归一化\n",
        "def normalization(data):\n",
        "    \"\"\"\n",
        "    data: original data with load\n",
        "    return: normalized data, scaler of load\n",
        "    \"\"\"\n",
        "    normalized_data = MinMaxScaler().fit_transform(data)\n",
        "    scaler_y = MinMaxScaler()\n",
        "    scaler_y.fit_transform(data[['load']])\n",
        "    return normalized_data, scaler_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tza1H7XMUF3K"
      },
      "source": [
        "# build supervised data\n",
        "def Series_To_Supervise(data, seq_len, target_len, y_col_index):\n",
        "    \"\"\"\n",
        "    convert series data to supervised data\n",
        "    :param data: original data\n",
        "    :param seq_len: length of sequence\n",
        "    :y_col_index: index of column which acts as output\n",
        "    :return: return two ndarrays-- input and output in format suitable to feed to LSTM\n",
        "    \"\"\"\n",
        "#     pdb.set_trace()\n",
        "    dim_0 = data.shape[0] - seq_len\n",
        "    dim_1 = data.shape[1]\n",
        "    x = np.zeros((dim_0, seq_len, dim_1))\n",
        "    y = np.zeros((dim_0, target_len))\n",
        "    for i in range(dim_0):\n",
        "        x[i] = data[i: i+seq_len]\n",
        "        y[i] = data[i+seq_len+1-target_len:i+seq_len+1, y_col_index]\n",
        "    print(\"shape of x: {}, shape of y: {}\".format(x.shape, y.shape))\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt4Wfny1UF3L"
      },
      "source": [
        "# 5-fold cross-validation\n",
        "def split_dataset(X, Y, n_split=5):\n",
        "    \"\"\"\n",
        "    X: original feature, size * 72 * features\n",
        "    Y: labels, size * 1\n",
        "    return: list of train_x, test_x, train_y, test_y\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_split, shuffle=True, random_state=1)\n",
        "    train_x_list = list()\n",
        "    valid_x_list = list()\n",
        "    test_x_list = list()\n",
        "    train_y_list = list()\n",
        "    valid_y_list = list()\n",
        "    test_y_list = list()\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        train_x_list.append(X[train_index])\n",
        "        train_y_list.append(Y[train_index])\n",
        "        test_x = X[test_index]\n",
        "        test_y = Y[test_index]\n",
        "        valid_x, test_x, valid_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=1)\n",
        "        valid_x_list.append(valid_x)\n",
        "        valid_y_list.append(valid_y)\n",
        "        test_x_list.append(test_x)\n",
        "        test_y_list.append(test_y)\n",
        "    return train_x_list, valid_x_list, test_x_list, train_y_list, valid_y_list, test_y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU0vS2KJG1R0"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7o87aIUG1R1"
      },
      "source": [
        "# define encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size=13, d_model=64, nhead=8, n_layers=4, dim_feedward=256):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Linear(input_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        encoder_input = self.pos_encoder(embedded)\n",
        "        output = self.transformer_encoder(encoder_input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUFb4F8jG1R1"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generate a square mask for the sequence. The masked positions are filled with float(‘-inf’). Unmasked positions are filled with float(0.0).\n",
        "    \"\"\"\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ULj_y9wG1R1"
      },
      "source": [
        "# define decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size=13, d_model=64, nhead=8, n_layers=4, dim_feedward=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Linear(input_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedward)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers)\n",
        "        self.out = nn.Linear(d_model, 1)\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        embedded = self.embedding(tgt)\n",
        "        decoder_input = self.pos_encoder(embedded)\n",
        "        output = self.transformer_decoder(decoder_input, memory, tgt_mask, memory_mask)\n",
        "        output = self.out(output)[-1,:]\n",
        "        return output        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w7k913XG1R2"
      },
      "source": [
        "# train the model \n",
        "def train_model(train_x, train_y, valid_x, valid_y, input_size=13, d_model=32, nhead=8, n_layers=4, dim_feedward=128,\n",
        "                seq_len=48, target_len=24, number_epochs=50, batch_size=512, teacher_forcing_ratio=0.5,\n",
        "                lr=0.01, training_prediction='teacher_forcing', dynamic_tf=False):\n",
        "    encoder = Encoder(input_size, d_model, nhead, n_layers, dim_feedward)\n",
        "    decoder = Decoder(input_size, d_model, nhead, n_layers, dim_feedward)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
        "    valid_dataset = TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
        "    criterion = nn.MSELoss()\n",
        "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    train_losses = list()\n",
        "    valid_loss_min = np.Inf\n",
        "    num_without_imp = 0\n",
        "    y_index = train_x.shape[2] - 1\n",
        "    \n",
        "    # train\n",
        "    for epoch in range(1, number_epochs + 1):\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.transpose(0, 1)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = torch.zeros(batch_size, target_len).to(device)\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "            # encoder\n",
        "            input_tensor = inputs[0:seq_len, :, :]\n",
        "            target_tensor = inputs[seq_len:, :, :]\n",
        "            memory = encoder(input_tensor)\n",
        "            # decoder\n",
        "            tgt = inputs[seq_len-1, :, :].unsqueeze(0)\n",
        "            \n",
        "            if training_prediction == 'recursive':\n",
        "                # predict recursively\n",
        "                for t in range(target_len):\n",
        "                    tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                    output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                    outputs[:, t] = output.squeeze()\n",
        "                    added_input = torch.cat((target_tensor[t, :, :-1].unsqueeze(0),\n",
        "                                             output.detach().unsqueeze(0)), 2)\n",
        "                    tgt = torch.cat((tgt, added_input), 0)\n",
        "            \n",
        "            if training_prediction == 'teacher_forcing':\n",
        "                # use teacher forcing\n",
        "                if random.random() < teacher_forcing_ratio:\n",
        "                    for t in range(target_len):\n",
        "                        tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                        output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                        outputs[:, t] = output.squeeze()\n",
        "                        added_input = target_tensor[t, :, :].unsqueeze(0)\n",
        "                        tgt = torch.cat((tgt, added_input), 0)\n",
        "                # predict recurisively\n",
        "                else:\n",
        "                    for t in range(target_len):\n",
        "                        tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                        output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                        outputs[:, t] = output.squeeze()\n",
        "                        added_input = torch.cat((target_tensor[t, :, :-1].unsqueeze(0),\n",
        "                                                 output.detach().unsqueeze(0)), 2)\n",
        "                        tgt = torch.cat((tgt, added_input), 0)\n",
        "            \n",
        "            if training_prediction == 'mixed_teacher_forcing':\n",
        "                # predict using mixed teacher forcing\n",
        "                for t in range(target_len):\n",
        "                    tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                    output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                    outputs[:, t] = output.squeeze()\n",
        "                    # predict with teacher forcing\n",
        "                    if random.random() < teacher_forcing_ratio:\n",
        "                        added_input = target_tensor[t, :, :].unsqueeze(0)\n",
        "                        tgt = torch.cat((tgt, added_input), 0)\n",
        "                            \n",
        "                    # predict recursively \n",
        "                    else:\n",
        "                        added_input = torch.cat((target_tensor[t, :, :-1].unsqueeze(0),\n",
        "                                                 output.detach().unsqueeze(0)), 2)\n",
        "                        tgt = torch.cat((tgt, added_input), 0)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_losses.append(loss.item)\n",
        "            loss.backward()\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "            \n",
        "            # eval\n",
        "            if i % 5 == 0:\n",
        "                if num_without_imp > 10:\n",
        "                    return encoder, decoder\n",
        "                num_without_imp = num_without_imp + 1\n",
        "                valid_losses = list()\n",
        "                encoder.eval()\n",
        "                decoder.eval()\n",
        "                for inp, lab in valid_loader:\n",
        "                    inp = inp.transpose(0, 1)\n",
        "                    inp = inp.to(device)\n",
        "                    lab = lab.to(device)\n",
        "                    out = torch.zeros(batch_size, target_len).to(device)\n",
        "                    # encoder\n",
        "                    input_tensor = inp[0:seq_len, :, :]\n",
        "                    target_tensor = inp[seq_len:, :, 0:-1]\n",
        "                    memory = encoder(input_tensor)\n",
        "                    # decoder\n",
        "                    tgt = inp[seq_len-1, :, :].unsqueeze(0)\n",
        "                    # predict recuisively\n",
        "                    for t in range(target_len):\n",
        "                        tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                        output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                        out[:, t] = output.squeeze()\n",
        "                        added_input = torch.cat((target_tensor[t, :, :].unsqueeze(0),\n",
        "                                                 output.detach().unsqueeze(0)), 2)\n",
        "                        tgt = torch.cat((tgt, added_input), 0)\n",
        "                    valid_loss = criterion(out, lab)\n",
        "                    valid_losses.append(valid_loss.item())\n",
        "                encoder.train()\n",
        "                decoder.train()\n",
        "                print(\"Epoch: {}/{}...\".format(epoch, number_epochs),\n",
        "                     \"Step: {}/{}...\".format(i+1, len(train_dataset)//batch_size),\n",
        "                     \"Loss: {}...\".format(loss.item()),\n",
        "                     \"Valid Loss: {}...\".format(np.mean(valid_losses)))\n",
        "                if np.mean(valid_losses) < valid_loss_min:\n",
        "                    num_without_imp = 0\n",
        "                    torch.save(encoder.state_dict(), \"encoder_state_dict.pt\")\n",
        "                    torch.save(decoder.state_dict(), \"decoder_state_dict.pt\")\n",
        "                    print(\"Valid loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(valid_loss_min, np.mean(valid_losses)))\n",
        "                    valid_loss_min = np.mean(valid_losses)\n",
        "    return encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hWjPxUPG1R7"
      },
      "source": [
        "# test the model\n",
        "def test_model(encoder, decoder, test_x, test_y, batch_size, seq_len, target_len, scaler):\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    encoder.load_state_dict(torch.load('encoder_state_dict.pt'))\n",
        "    decoder.load_state_dict(torch.load('decoder_state_dict.pt'))\n",
        "    y_pred = list()\n",
        "    y_true = list()\n",
        "    y_index = test_x.shape[2] - 1\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.transpose(0, 1)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = torch.zeros(batch_size, target_len).to(device)\n",
        "            # encoder\n",
        "            input_tensor = inputs[0:seq_len, :, :]\n",
        "            target_tensor = inputs[seq_len:, :, :]\n",
        "            memory = encoder(input_tensor)\n",
        "            # decoder\n",
        "            tgt = inputs[seq_len-1, :, :].unsqueeze(0)\n",
        "            # predict recursively\n",
        "            for t in range(target_len):\n",
        "                tgt_mask = generate_square_subsequent_mask(tgt.shape[0])\n",
        "                output = decoder(tgt, memory, tgt_mask=tgt_mask)\n",
        "                outputs[:, t] = output.squeeze()\n",
        "                added_input = torch.cat((target_tensor[t, :, :-1].unsqueeze(0),\n",
        "                                         output.detach().unsqueeze(0)), 2)\n",
        "                tgt = torch.cat((tgt, added_input), 0)\n",
        "            y_pred = y_pred + outputs.view(-1).cpu().numpy().tolist()\n",
        "            y_true = y_true + labels.view(-1).cpu().numpy().tolist()\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    load_true = scaler.inverse_transform(np.expand_dims(y_true, axis=1))\n",
        "    load_pred = scaler.inverse_transform(np.expand_dims(y_pred, axis=1))\n",
        "    MAPE = np.mean(np.abs(load_true-load_pred)/load_true)\n",
        "    return MAPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thj9qZovUF3N"
      },
      "source": [
        "# model configs\n",
        "def Model_Configs():\n",
        "    batch_sizes = [512]\n",
        "    n_layers = [1]\n",
        "    lrs = [0.01]\n",
        "    hidden_dims = [70]\n",
        "    configs = list()\n",
        "    for i in batch_sizes:\n",
        "        for j in n_layers:\n",
        "            for k in lrs:\n",
        "                for l in hidden_dims:\n",
        "                    configs.append([i, j, k, l])\n",
        "    return configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L03esL1dUF3O"
      },
      "source": [
        "def main(n_split=5, seq_len=48, target_len=24):\n",
        "    normalized_data, scaler_y = normalization(data)\n",
        "    y_index = normalized_data.shape[1] - 1\n",
        "    num_features = normalized_data.shape[1]\n",
        "    x, y = Series_To_Supervise(normalized_data, seq_len=seq_len + target_len, target_len=target_len, y_col_index=y_index)\n",
        "    train_x_list, valid_x_list, test_x_list, train_y_list, valid_y_list, test_y_list = split_dataset(x, y, n_split=n_split)    \n",
        "    print(\"model configs set.\")\n",
        "    configs = Model_Configs()\n",
        "    MAPE_list = list()\n",
        "    for config in configs:\n",
        "        batch_size = config[0]\n",
        "        n_layer = config[1]\n",
        "        lr = config[2]\n",
        "        hidden_dim = config[3]\n",
        "        print(\"Config: batch_size--{}, n_layer--{}, lr--{}, hidden_dims--{}\".format(batch_size, n_layer, lr, hidden_dim))\n",
        "        tmp_list = list()\n",
        "        tmp_list_24 = list()\n",
        "        for i in range(n_split):\n",
        "            encoder, decoder = train_model(train_x_list[i], train_y_list[i], valid_x_list[i], valid_y_list[i],\n",
        "                                           input_size=num_features, batch_size=batch_size, lr=lr, seq_len=seq_len, \n",
        "                                           target_len=target_len, training_prediction='teacher_forcing')\n",
        "            MAPE = test_model(encoder, decoder, test_x_list[i], test_y_list[i], batch_size=batch_size, seq_len=seq_len,\n",
        "                              target_len=target_len, scaler=scaler_y)\n",
        "            if MAPE < 0.1:\n",
        "                break\n",
        "            tmp_list.append(MAPE)\n",
        "        MAPE_list.append(tmp_list)\n",
        "    return MAPE_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgD1KKyhMQOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea80bc4b-a7af-4431-bf36-281ed4d10419"
      },
      "source": [
        "MAPE_list = main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x: (22745, 72, 6), shape of y: (22745, 24)\n",
            "model configs set.\n",
            "Config: batch_size--512, n_layer--1, lr--0.01, hidden_dims--70\n",
            "Epoch: 1/50... Step: 1/35... Loss: 0.36865609884262085... Valid Loss: 5.518253564834595...\n",
            "Valid loss decreased (inf --> 5.518254).  Saving model ...\n",
            "Epoch: 1/50... Step: 6/35... Loss: 0.06205814331769943... Valid Loss: 0.05728647764772177...\n",
            "Valid loss decreased (5.518254 --> 0.057286).  Saving model ...\n",
            "Epoch: 1/50... Step: 11/35... Loss: 0.030360614880919456... Valid Loss: 0.02581830369308591...\n",
            "Valid loss decreased (0.057286 --> 0.025818).  Saving model ...\n",
            "Epoch: 1/50... Step: 16/35... Loss: 0.04631965979933739... Valid Loss: 0.03129823878407478...\n",
            "Epoch: 1/50... Step: 21/35... Loss: 0.036569248884916306... Valid Loss: 0.035850342363119125...\n",
            "Epoch: 1/50... Step: 26/35... Loss: 0.028457563370466232... Valid Loss: 0.020985573064535856...\n",
            "Valid loss decreased (0.025818 --> 0.020986).  Saving model ...\n",
            "Epoch: 1/50... Step: 31/35... Loss: 0.0277556162327528... Valid Loss: 0.024170286022126675...\n",
            "Epoch: 2/50... Step: 1/35... Loss: 0.022367730736732483... Valid Loss: 0.021683691069483757...\n",
            "Epoch: 2/50... Step: 6/35... Loss: 0.027101648971438408... Valid Loss: 0.0225923927500844...\n",
            "Epoch: 2/50... Step: 11/35... Loss: 0.023868143558502197... Valid Loss: 0.021128463093191385...\n",
            "Epoch: 2/50... Step: 16/35... Loss: 0.024721406400203705... Valid Loss: 0.02135461987927556...\n",
            "Epoch: 2/50... Step: 21/35... Loss: 0.023794233798980713... Valid Loss: 0.02185681276023388...\n",
            "Epoch: 2/50... Step: 26/35... Loss: 0.025481048971414566... Valid Loss: 0.021477340254932642...\n",
            "Epoch: 2/50... Step: 31/35... Loss: 0.024088338017463684... Valid Loss: 0.021261562127619982...\n",
            "Epoch: 3/50... Step: 1/35... Loss: 0.023182226344943047... Valid Loss: 0.02151624020189047...\n",
            "Epoch: 3/50... Step: 6/35... Loss: 0.023752465844154358... Valid Loss: 0.021546402014791965...\n",
            "Epoch: 3/50... Step: 11/35... Loss: 0.0240012239664793... Valid Loss: 0.02147669717669487...\n",
            "Epoch: 1/50... Step: 1/35... Loss: 0.10274291038513184... Valid Loss: 7.01309597492218...\n",
            "Valid loss decreased (inf --> 7.013096).  Saving model ...\n",
            "Epoch: 1/50... Step: 6/35... Loss: 0.4037466049194336... Valid Loss: 0.20888782292604446...\n",
            "Valid loss decreased (7.013096 --> 0.208888).  Saving model ...\n",
            "Epoch: 1/50... Step: 11/35... Loss: 0.09071160852909088... Valid Loss: 0.07756992429494858...\n",
            "Valid loss decreased (0.208888 --> 0.077570).  Saving model ...\n",
            "Epoch: 1/50... Step: 16/35... Loss: 0.022846346721053123... Valid Loss: 0.02874023001641035...\n",
            "Valid loss decreased (0.077570 --> 0.028740).  Saving model ...\n",
            "Epoch: 1/50... Step: 21/35... Loss: 0.037337664514780045... Valid Loss: 0.0312057975679636...\n",
            "Epoch: 1/50... Step: 26/35... Loss: 0.025079786777496338... Valid Loss: 0.024857753422111273...\n",
            "Valid loss decreased (0.028740 --> 0.024858).  Saving model ...\n",
            "Epoch: 1/50... Step: 31/35... Loss: 0.02914307825267315... Valid Loss: 0.02426594542339444...\n",
            "Valid loss decreased (0.024858 --> 0.024266).  Saving model ...\n",
            "Epoch: 2/50... Step: 1/35... Loss: 0.02480277419090271... Valid Loss: 0.02346720965579152...\n",
            "Valid loss decreased (0.024266 --> 0.023467).  Saving model ...\n",
            "Epoch: 2/50... Step: 6/35... Loss: 0.02452300488948822... Valid Loss: 0.023283769842237234...\n",
            "Valid loss decreased (0.023467 --> 0.023284).  Saving model ...\n",
            "Epoch: 2/50... Step: 11/35... Loss: 0.0224313884973526... Valid Loss: 0.021746600046753883...\n",
            "Valid loss decreased (0.023284 --> 0.021747).  Saving model ...\n",
            "Epoch: 2/50... Step: 16/35... Loss: 0.02162832021713257... Valid Loss: 0.02200058987364173...\n",
            "Epoch: 2/50... Step: 21/35... Loss: 0.02230447344481945... Valid Loss: 0.022126173600554466...\n",
            "Epoch: 2/50... Step: 26/35... Loss: 0.02294495329260826... Valid Loss: 0.02201059414073825...\n",
            "Epoch: 2/50... Step: 31/35... Loss: 0.021916508674621582... Valid Loss: 0.021912928204983473...\n",
            "Epoch: 3/50... Step: 1/35... Loss: 0.023840509355068207... Valid Loss: 0.02164844237267971...\n",
            "Valid loss decreased (0.021747 --> 0.021648).  Saving model ...\n",
            "Epoch: 3/50... Step: 6/35... Loss: 0.022220440208911896... Valid Loss: 0.02165749203413725...\n",
            "Epoch: 3/50... Step: 11/35... Loss: 0.02406216785311699... Valid Loss: 0.02166074514389038...\n",
            "Epoch: 3/50... Step: 16/35... Loss: 0.02335875853896141... Valid Loss: 0.02196661476045847...\n",
            "Epoch: 3/50... Step: 21/35... Loss: 0.02468397468328476... Valid Loss: 0.02221672609448433...\n",
            "Epoch: 3/50... Step: 26/35... Loss: 0.022263385355472565... Valid Loss: 0.02161490311846137...\n",
            "Valid loss decreased (0.021648 --> 0.021615).  Saving model ...\n",
            "Epoch: 3/50... Step: 31/35... Loss: 0.024642398580908775... Valid Loss: 0.02207566099241376...\n",
            "Epoch: 4/50... Step: 1/35... Loss: 0.02319205179810524... Valid Loss: 0.022052730433642864...\n",
            "Epoch: 4/50... Step: 6/35... Loss: 0.021835923194885254... Valid Loss: 0.02218565810471773...\n",
            "Epoch: 4/50... Step: 11/35... Loss: 0.023045197129249573... Valid Loss: 0.02152669196948409...\n",
            "Valid loss decreased (0.021615 --> 0.021527).  Saving model ...\n",
            "Epoch: 4/50... Step: 16/35... Loss: 0.024820156395435333... Valid Loss: 0.021763515658676624...\n",
            "Epoch: 4/50... Step: 21/35... Loss: 0.0222993865609169... Valid Loss: 0.02160024270415306...\n",
            "Epoch: 4/50... Step: 26/35... Loss: 0.024557575583457947... Valid Loss: 0.021948958281427622...\n",
            "Epoch: 4/50... Step: 31/35... Loss: 0.022176753729581833... Valid Loss: 0.021710703149437904...\n",
            "Epoch: 5/50... Step: 1/35... Loss: 0.021910827606916428... Valid Loss: 0.021931137889623642...\n",
            "Epoch: 5/50... Step: 6/35... Loss: 0.022004103288054466... Valid Loss: 0.02163176517933607...\n",
            "Epoch: 5/50... Step: 11/35... Loss: 0.020943135023117065... Valid Loss: 0.02163586625829339...\n",
            "Epoch: 5/50... Step: 16/35... Loss: 0.02364225685596466... Valid Loss: 0.0216258829459548...\n",
            "Epoch: 5/50... Step: 21/35... Loss: 0.02164801023900509... Valid Loss: 0.021703932899981737...\n",
            "Epoch: 5/50... Step: 26/35... Loss: 0.022288180887699127... Valid Loss: 0.02171384170651436...\n",
            "Epoch: 5/50... Step: 31/35... Loss: 0.021247845143079758... Valid Loss: 0.02176792360842228...\n",
            "Epoch: 1/50... Step: 1/35... Loss: 0.09684668481349945... Valid Loss: 5.5745813846588135...\n",
            "Valid loss decreased (inf --> 5.574581).  Saving model ...\n",
            "Epoch: 1/50... Step: 6/35... Loss: 0.06339257210493088... Valid Loss: 0.030926839914172888...\n",
            "Valid loss decreased (5.574581 --> 0.030927).  Saving model ...\n",
            "Epoch: 1/50... Step: 11/35... Loss: 0.025720249861478806... Valid Loss: 0.041820292361080647...\n",
            "Epoch: 1/50... Step: 16/35... Loss: 0.02597443386912346... Valid Loss: 0.021029393654316664...\n",
            "Valid loss decreased (0.030927 --> 0.021029).  Saving model ...\n",
            "Epoch: 1/50... Step: 21/35... Loss: 0.03025795891880989... Valid Loss: 0.023540060967206955...\n",
            "Epoch: 1/50... Step: 26/35... Loss: 0.026597056537866592... Valid Loss: 0.027443156577646732...\n",
            "Epoch: 1/50... Step: 31/35... Loss: 0.024559490382671356... Valid Loss: 0.02136513451114297...\n",
            "Epoch: 2/50... Step: 1/35... Loss: 0.025344233959913254... Valid Loss: 0.022183988709002733...\n",
            "Epoch: 2/50... Step: 6/35... Loss: 0.023860111832618713... Valid Loss: 0.023038055282086134...\n",
            "Epoch: 2/50... Step: 11/35... Loss: 0.02292606793344021... Valid Loss: 0.02123185433447361...\n",
            "Epoch: 2/50... Step: 16/35... Loss: 0.02317081019282341... Valid Loss: 0.021554362028837204...\n",
            "Epoch: 2/50... Step: 21/35... Loss: 0.023101218044757843... Valid Loss: 0.021506025455892086...\n",
            "Epoch: 2/50... Step: 26/35... Loss: 0.023617446422576904... Valid Loss: 0.02121885260567069...\n",
            "Epoch: 2/50... Step: 31/35... Loss: 0.024737423285841942... Valid Loss: 0.020932006184011698...\n",
            "Valid loss decreased (0.021029 --> 0.020932).  Saving model ...\n",
            "Epoch: 3/50... Step: 1/35... Loss: 0.023925339803099632... Valid Loss: 0.02163244690746069...\n",
            "Epoch: 3/50... Step: 6/35... Loss: 0.021596405655145645... Valid Loss: 0.02118654176592827...\n",
            "Epoch: 3/50... Step: 11/35... Loss: 0.024365326389670372... Valid Loss: 0.021291224285960197...\n",
            "Epoch: 3/50... Step: 16/35... Loss: 0.02188868634402752... Valid Loss: 0.022020873613655567...\n",
            "Epoch: 3/50... Step: 21/35... Loss: 0.021810069680213928... Valid Loss: 0.02136717038229108...\n",
            "Epoch: 3/50... Step: 26/35... Loss: 0.023679276928305626... Valid Loss: 0.021183330565690994...\n",
            "Epoch: 3/50... Step: 31/35... Loss: 0.022918932139873505... Valid Loss: 0.02161005837842822...\n",
            "Epoch: 4/50... Step: 1/35... Loss: 0.02313898503780365... Valid Loss: 0.021368091460317373...\n",
            "Epoch: 4/50... Step: 6/35... Loss: 0.021939881145954132... Valid Loss: 0.021305819507688284...\n",
            "Epoch: 4/50... Step: 11/35... Loss: 0.022437430918216705... Valid Loss: 0.021847534459084272...\n",
            "Epoch: 4/50... Step: 16/35... Loss: 0.022816460579633713... Valid Loss: 0.02109215222299099...\n",
            "Epoch: 1/50... Step: 1/35... Loss: 0.09996872395277023... Valid Loss: 3.321562945842743...\n",
            "Valid loss decreased (inf --> 3.321563).  Saving model ...\n",
            "Epoch: 1/50... Step: 6/35... Loss: 0.04906810075044632... Valid Loss: 0.03001442877575755...\n",
            "Valid loss decreased (3.321563 --> 0.030014).  Saving model ...\n",
            "Epoch: 1/50... Step: 11/35... Loss: 0.029556699097156525... Valid Loss: 0.023083302192389965...\n",
            "Valid loss decreased (0.030014 --> 0.023083).  Saving model ...\n",
            "Epoch: 1/50... Step: 16/35... Loss: 0.02899467945098877... Valid Loss: 0.02265154803171754...\n",
            "Valid loss decreased (0.023083 --> 0.022652).  Saving model ...\n",
            "Epoch: 1/50... Step: 21/35... Loss: 0.024756353348493576... Valid Loss: 0.022808434441685677...\n",
            "Epoch: 1/50... Step: 26/35... Loss: 0.025471076369285583... Valid Loss: 0.024035813752561808...\n",
            "Epoch: 1/50... Step: 31/35... Loss: 0.022284582257270813... Valid Loss: 0.02214648760855198...\n",
            "Valid loss decreased (0.022652 --> 0.022146).  Saving model ...\n",
            "Epoch: 2/50... Step: 1/35... Loss: 0.02390098199248314... Valid Loss: 0.02236314071342349...\n",
            "Epoch: 2/50... Step: 6/35... Loss: 0.023127421736717224... Valid Loss: 0.02253342792391777...\n",
            "Epoch: 2/50... Step: 11/35... Loss: 0.0244358628988266... Valid Loss: 0.02214783802628517...\n",
            "Epoch: 2/50... Step: 16/35... Loss: 0.02227269671857357... Valid Loss: 0.022410265635699034...\n",
            "Epoch: 2/50... Step: 21/35... Loss: 0.024223238229751587... Valid Loss: 0.022368479054421186...\n",
            "Epoch: 2/50... Step: 26/35... Loss: 0.022582633420825005... Valid Loss: 0.02220143750309944...\n",
            "Epoch: 2/50... Step: 31/35... Loss: 0.02211291901767254... Valid Loss: 0.021658004727214575...\n",
            "Valid loss decreased (0.022146 --> 0.021658).  Saving model ...\n",
            "Epoch: 3/50... Step: 1/35... Loss: 0.02076422981917858... Valid Loss: 0.02204798860475421...\n",
            "Epoch: 3/50... Step: 6/35... Loss: 0.02325393818318844... Valid Loss: 0.02205274999141693...\n",
            "Epoch: 3/50... Step: 11/35... Loss: 0.022445445880293846... Valid Loss: 0.022136207204312086...\n",
            "Epoch: 3/50... Step: 16/35... Loss: 0.024965092539787292... Valid Loss: 0.022449751384556293...\n",
            "Epoch: 3/50... Step: 21/35... Loss: 0.02002231776714325... Valid Loss: 0.021802261471748352...\n",
            "Epoch: 3/50... Step: 26/35... Loss: 0.02247202955186367... Valid Loss: 0.022013303358107805...\n",
            "Epoch: 3/50... Step: 31/35... Loss: 0.019866103306412697... Valid Loss: 0.02215681876987219...\n",
            "Epoch: 4/50... Step: 1/35... Loss: 0.02354133129119873... Valid Loss: 0.022074509877711535...\n",
            "Epoch: 4/50... Step: 6/35... Loss: 0.022785421460866928... Valid Loss: 0.022380941081792116...\n",
            "Epoch: 4/50... Step: 11/35... Loss: 0.021090399473905563... Valid Loss: 0.022194592747837305...\n",
            "Epoch: 4/50... Step: 16/35... Loss: 0.02148592099547386... Valid Loss: 0.02193963946774602...\n",
            "Epoch: 1/50... Step: 1/35... Loss: 0.4276244044303894... Valid Loss: 4.652778148651123...\n",
            "Valid loss decreased (inf --> 4.652778).  Saving model ...\n",
            "Epoch: 1/50... Step: 6/35... Loss: 0.06425212323665619... Valid Loss: 0.15741299465298653...\n",
            "Valid loss decreased (4.652778 --> 0.157413).  Saving model ...\n",
            "Epoch: 1/50... Step: 11/35... Loss: 0.03417219594120979... Valid Loss: 0.053396561183035374...\n",
            "Valid loss decreased (0.157413 --> 0.053397).  Saving model ...\n",
            "Epoch: 1/50... Step: 16/35... Loss: 0.0369560569524765... Valid Loss: 0.021489090751856565...\n",
            "Valid loss decreased (0.053397 --> 0.021489).  Saving model ...\n",
            "Epoch: 1/50... Step: 21/35... Loss: 0.038104165345430374... Valid Loss: 0.03275675978511572...\n",
            "Epoch: 1/50... Step: 26/35... Loss: 0.027939554303884506... Valid Loss: 0.02195520419627428...\n",
            "Epoch: 1/50... Step: 31/35... Loss: 0.028069134801626205... Valid Loss: 0.02185317873954773...\n",
            "Epoch: 2/50... Step: 1/35... Loss: 0.025210505351424217... Valid Loss: 0.023972198832780123...\n",
            "Epoch: 2/50... Step: 6/35... Loss: 0.02486555278301239... Valid Loss: 0.021522308234125376...\n",
            "Epoch: 2/50... Step: 11/35... Loss: 0.023093268275260925... Valid Loss: 0.021451981738209724...\n",
            "Valid loss decreased (0.021489 --> 0.021452).  Saving model ...\n",
            "Epoch: 2/50... Step: 16/35... Loss: 0.024860650300979614... Valid Loss: 0.02181304944679141...\n",
            "Epoch: 2/50... Step: 21/35... Loss: 0.026447853073477745... Valid Loss: 0.022381947375833988...\n",
            "Epoch: 2/50... Step: 26/35... Loss: 0.025036534294486046... Valid Loss: 0.021156687289476395...\n",
            "Valid loss decreased (0.021452 --> 0.021157).  Saving model ...\n",
            "Epoch: 2/50... Step: 31/35... Loss: 0.025289099663496017... Valid Loss: 0.02102427463978529...\n",
            "Valid loss decreased (0.021157 --> 0.021024).  Saving model ...\n",
            "Epoch: 3/50... Step: 1/35... Loss: 0.023217875510454178... Valid Loss: 0.0219941814430058...\n",
            "Epoch: 3/50... Step: 6/35... Loss: 0.023875348269939423... Valid Loss: 0.02114222152158618...\n",
            "Epoch: 3/50... Step: 11/35... Loss: 0.022767912596464157... Valid Loss: 0.02148087089881301...\n",
            "Epoch: 3/50... Step: 16/35... Loss: 0.024030989035964012... Valid Loss: 0.02177564986050129...\n",
            "Epoch: 3/50... Step: 21/35... Loss: 0.022377919405698776... Valid Loss: 0.021276131737977266...\n",
            "Epoch: 3/50... Step: 26/35... Loss: 0.024803027510643005... Valid Loss: 0.021664998959749937...\n",
            "Epoch: 3/50... Step: 31/35... Loss: 0.025713635608553886... Valid Loss: 0.021689495537430048...\n",
            "Epoch: 4/50... Step: 1/35... Loss: 0.021424993872642517... Valid Loss: 0.021463043056428432...\n",
            "Epoch: 4/50... Step: 6/35... Loss: 0.0237530879676342... Valid Loss: 0.021735547110438347...\n",
            "Epoch: 4/50... Step: 11/35... Loss: 0.024060267955064774... Valid Loss: 0.021204191725701094...\n",
            "Epoch: 4/50... Step: 16/35... Loss: 0.020920326933264732... Valid Loss: 0.02132866159081459...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzDgygZylJV1",
        "outputId": "e4f528d4-ffd0-4235-b194-272bcac2e5df"
      },
      "source": [
        "MAPE_list"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.20634211810239275,\n",
              "  0.19461447828968573,\n",
              "  0.19300887313337248,\n",
              "  0.19481059123145575,\n",
              "  0.20026291033115215]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "X7X1-grJUF3Q",
        "outputId": "b285afec-d6ff-422e-9716-8299dfbe8a78"
      },
      "source": [
        "configs = Model_Configs()\n",
        "best_config = configs[np.argmin(MAPE_list)]\n",
        "for i in range(len(configs)):\n",
        "    print(\"config: {}, MAPE: {:.6f}\".format(configs[i], MAPE_list[i]))\n",
        "print(\"best config: {}, MAPE: {:.6f}\".format(best_config, min(MAPE_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a4dbe3fd1230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAPE_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config: {}, MAPE: {:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAPE_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best config: {}, MAPE: {:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAPE_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}